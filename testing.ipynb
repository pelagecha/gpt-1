{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1: bag of words\n",
    "xbow = torch.zeros((B,T,C)) # bag of words\n",
    "for b in range(B): # iterative approach\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "         [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]),\n",
       " tensor([[ 0.6245, -1.9711],\n",
       "         [-0.0555, -1.4861],\n",
       "         [ 0.1136, -1.0268],\n",
       "         [-0.4454, -0.4890],\n",
       "         [-0.5955, -0.0595],\n",
       "         [-0.3372, -0.1447],\n",
       "         [-0.4577,  0.1969],\n",
       "         [-0.3316,  0.0153]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: matrix multiply\n",
    "wei = torch.tril(torch.ones(T, T)) # parrarel approach (low left triangular matrix with rows summing up to 1)\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) -> (B, T, C)\n",
    "wei, xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: SoftMax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "wei, xbow3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 8, 16]),\n",
       " tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.5054e-01, 4.9456e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [1.6184e-01, 6.3264e-01, 2.0551e-01, 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [5.3194e-04, 1.6164e-05, 9.9763e-01, 1.8206e-03, 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.9920e-01, 3.4292e-07, 5.1411e-06, 2.2355e-07, 7.9542e-04,\n",
       "           0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.4315e-06, 9.8354e-01, 1.4112e-02, 6.3690e-04, 1.6176e-03,\n",
       "           8.7480e-05, 0.0000e+00, 0.0000e+00],\n",
       "          [3.1648e-05, 9.8119e-01, 2.1116e-05, 6.1659e-08, 1.8755e-02,\n",
       "           8.0306e-07, 3.5688e-07, 0.0000e+00],\n",
       "          [5.4150e-06, 7.4348e-06, 2.7379e-03, 3.5637e-08, 9.9164e-01,\n",
       "           5.4801e-03, 8.3817e-05, 4.0323e-05]]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: (masked) self-attention !!\n",
    "B, T, C = 4,8,32\n",
    "x = torch.randn(B, T, C)\n",
    "tril = torch.tril(torch.ones(T, T)) # lower triangular structure\n",
    "\n",
    "head_size = 16\n",
    "key   = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1)                 # Key @ Query: (B,T,16) @ (T,B,16) -> (B, T, T)\n",
    "wei = wei.masked_fill(tril==0, float(\"-inf\")) # infinities are \"not allowed\" to communicate, so \"Masked self-attention\", NOTE: remove this line for encoder!\n",
    "wei /= head_size**-0.5                        # making the variance be 1\n",
    "wei = F.softmax(wei, dim=-1)                  # average out, otherwise softmax may converge to one-hot vectors, which waste space, computation and may lead to dead neurons\n",
    "out = wei @ v\n",
    "out.shape, wei[:1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
